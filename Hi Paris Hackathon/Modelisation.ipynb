{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1d53c30-82fe-4772-aa18-7f6e6fdc8e46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#to run if necessary\n",
    "#!pip install catboost\n",
    "#!pip install xgboost\n",
    "#!pip install lightgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "567742c2-48e8-424e-a6aa-33d65932bad5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, r2_score\n",
    "from catboost import CatBoostRegressor\n",
    "from  xgboost import XGBRegressor \n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import math\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, r2_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_predict\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4352e6b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Charger votre DataFrame\n",
    "df = pd.read_csv('datasets/yann_train_avec_index.csv', sep=';')\n",
    "\n",
    "# Séparer les features et la target\n",
    "X = df.drop(['index','Unnamed: 0', 'Month 4','Third_T1','Third_T2','Third_T3'], axis=1)\n",
    "y = df['Month 4']\n",
    "\n",
    "# Effectuer le train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Création de l'imputateur\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Imputation sur les ensembles d'entraînement et de test\n",
    "X_train_imputed = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns)\n",
    "X_test_imputed = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "# Recombiner les features et les targets pour le train et le test\n",
    "train = pd.concat([X_train_imputed, y_train.reset_index(drop=True)], axis=1)\n",
    "test = pd.concat([X_test_imputed, y_test.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Sauvegarder les ensembles de données train et test en format CSV\n",
    "train.to_csv('datasets/yann/train_data.csv', index=False)\n",
    "test.to_csv('datasets/yann/test_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###     1.1. Standardize train and test before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45204cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Assume 'train' and 'test' are your initial DataFrames and 'Month 4' is a column not an index\n",
    "\n",
    "# Separate features and target variable for training data\n",
    "X_train = train.drop('Month 4', axis=1)\n",
    "y_train = train['Month 4'].copy()\n",
    "\n",
    "# Separate features and target variable for test data\n",
    "X_test = test.drop('Month 4', axis=1)\n",
    "y_test = test['Month 4'].copy()\n",
    "\n",
    "# Fit the scaler on the training features\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Scale the training features\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "\n",
    "# Scale the test features\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert the scaled arrays back into DataFrames\n",
    "X_train = pd.DataFrame(X_train_scaled, index=train.index, columns=X_train.columns)\n",
    "X_test = pd.DataFrame(X_test_scaled, index=test.index, columns=X_test.columns)\n",
    "\n",
    "# Concatenate the scaled features with the target variable\n",
    "train = pd.concat([X_train, y_train], axis=1)\n",
    "test = pd.concat([X_test, y_test], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. CREATION ON THE MODELS AND MODEL SELECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1. creation of function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "daf04a87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "import math\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, r2_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Importez ici les modèles spécifiques que vous utilisez\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def rmse_score(y_true, y_pred):\n",
    "    return mean_squared_error(y_true, y_pred, squared=False)\n",
    "\n",
    "def RMSE_hfactory(y_true, y_pred):\n",
    "    # Calculez R_0 en utilisant la valeur vraie et une liste de zéros\n",
    "    R_0 = rmse_score(y_true, [0]*len(y_true))\n",
    "    \n",
    "    # Calculez RMSE_pred en utilisant la valeur vraie et les prédictions du modèle\n",
    "    RMSE_pred = rmse_score(y_true, y_pred)\n",
    "    \n",
    "    # Calculez RMSE_hfactory en utilisant la formule donnée\n",
    "    RMSE_hfactory_value = (R_0 - 0.8 * RMSE_pred) / R_0\n",
    "    \n",
    "    return RMSE_hfactory_value\n",
    "\n",
    "\n",
    "def train_prediction_models(data_path_suffixe, train, test, model_names, models, hyperparameters):\n",
    "    not_keep = ['Unnamed: 0','Month 4']\n",
    "    \n",
    "    print(\"--------------------------------- \\n\" +\n",
    "          \"TRAINING IN PROCESS  : \" + data_path_suffixe\n",
    "          )\n",
    "\n",
    "    # Récupération des hyper-paramètres optimaux\n",
    "    best_params = {}\n",
    "\n",
    "    # Variables utilisées dans le modèle\n",
    "    predictors = [p for p in train.columns if p not in not_keep]\n",
    "    target = 'Month 4'\n",
    "\n",
    "    # Split train / test\n",
    "    X_train = train[predictors]\n",
    "    y_train = train[target]\n",
    "\n",
    "    X_test = test[predictors]\n",
    "    y_test = test[target]\n",
    "\n",
    "    # Entraînement et prédiction des modèles\n",
    "    for model_name in model_names:\n",
    "        print('----------------------------------------------------')\n",
    "        print(model_name)\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        model = models[model_name]\n",
    "        hyperparameters_model = hyperparameters[model_name]\n",
    "        search = GridSearchCV(model, hyperparameters_model, cv=5, scoring=make_scorer(rmse_score, greater_is_better=False), verbose=0, n_jobs=-1)\n",
    "        search.fit(X_train, y_train)\n",
    "\n",
    "        best_estimator = search.best_estimator_\n",
    "        print('Hyperparameters:', search.best_params_)\n",
    "\n",
    "        # Sauvegarde des hyperparamètres optimaux\n",
    "        for param in search.best_params_:\n",
    "            best_params[model_name + '__' + param] = search.best_params_[param]\n",
    "\n",
    "        # Prédiction et évaluation\n",
    "        y_pred_train = best_estimator.predict(X_train)\n",
    "        y_pred_test = best_estimator.predict(X_test)\n",
    "        mse_train = rmse_score(y_train, y_pred_train)\n",
    "        mse_test = rmse_score(y_test, y_pred_test)\n",
    "        \n",
    "        rmseh_train = RMSE_hfactory(y_train, y_pred_train)\n",
    "        rmseh_test = RMSE_hfactory(y_test, y_pred_test)\n",
    "\n",
    "        print('train RMSE Hfactory:', rmseh_train)\n",
    "        print('test RMSE Hfactory :', rmseh_test)\n",
    "\n",
    "        print('Train RMSE:', mse_train)\n",
    "        print('Test RMSE:', mse_test)\n",
    "\n",
    "        end = time.time()\n",
    "        print('Execution time:', math.ceil((end - start) / 60), 'min')\n",
    "        print('----------------------------------------------------')\n",
    "        print(' ')\n",
    "\n",
    "        # Génération des noms de fichiers avec un compteur en cas de fichiers existants\n",
    "        counter = 0\n",
    "        while True:\n",
    "            model_filename = f'model_{model_name}_{counter}.pkl'\n",
    "            params_filename = f'hyperparameters_{model_name}_{counter}.txt'\n",
    "\n",
    "            # Vérifier si les fichiers existent déjà\n",
    "            if not os.path.exists(model_filename) and not os.path.exists(params_filename):\n",
    "                # Sauvegarde du modèle\n",
    "                with open(model_filename, 'wb') as file:\n",
    "                    pickle.dump(best_estimator, file)\n",
    "                \n",
    "                # Sauvegarde des hyperparamètres\n",
    "                with open(params_filename, 'w') as file:\n",
    "                    file.write('Best Hyperparameters for ' + model_name + ':\\n')\n",
    "                    file.write(str(search.best_params_))\n",
    "                break\n",
    "            else:\n",
    "                counter += 1\n",
    "    # Résumé des hyper-paramètres optimaux des modèles\n",
    "    print('Optimal hyperparameters of models')\n",
    "    print(best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Test of different models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5c910f",
   "metadata": {},
   "source": [
    "# Tous les modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9f7faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------- \n",
      "TRAINING IN PROCESS FOR DISEASE : dataset/data\n",
      "----------------------------------------------------\n",
      "LinearRegression\n",
      "Hyperparameters: {}\n",
      "Train MSE: 297957.3150024991\n",
      "Test MSE: 376335.60683266533\n",
      "Execution time: 1 min\n",
      "----------------------------------------------------\n",
      " \n",
      "----------------------------------------------------\n",
      "CatBoost\n",
      "Hyperparameters: {'learning_rate': 0.001, 'n_estimators': 2500}\n",
      "Train MSE: 242961.55378075698\n",
      "Test MSE: 438594.50243551907\n",
      "Execution time: 266 min\n",
      "----------------------------------------------------\n",
      " \n",
      "----------------------------------------------------\n",
      "XGBoost\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, VotingRegressor, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Model names\n",
    "model_names = ['LinearRegression', 'CatBoost', 'XGBoost', 'RandomForest', 'DecisionTree', 'VotingRegressor']\n",
    "\n",
    "# Models used\n",
    "models = {\n",
    "    'CatBoost': CatBoostRegressor(verbose=False, random_state=seed),\n",
    "    'XGBoost': XGBRegressor(verbose=False, random_state=seed),\n",
    "    'RandomForest': RandomForestRegressor(verbose=False, random_state=seed),\n",
    "    'DecisionTree': DecisionTreeRegressor(random_state=seed),\n",
    "    'LinearRegression': LinearRegression(),\n",
    "    'VotingRegressor': VotingRegressor(estimators=[\n",
    "        ('CatBoost', CatBoostRegressor(verbose=False, random_state=seed)), \n",
    "        ('XGBoost', XGBRegressor(verbose=False, random_state=seed)), \n",
    "        ('RandomForest', RandomForestRegressor(verbose=False, random_state=seed))\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Hyperparameter grids\n",
    "hyperparameters = {\n",
    "    'CatBoost': {'n_estimators': list(range(1000, 2550, 150)), 'learning_rate': [0.001, 0.01, 0.05, 0.1]},\n",
    "    'XGBoost': {'n_estimators': list(range(1000, 1500, 150)), 'learning_rate': [0.001]},\n",
    "    'RandomForest': {'n_estimators': list(range(1000, 2550, 150)), 'max_depth': list(range(2, 11)) + [None]},\n",
    "    'DecisionTree': {'max_depth': list(range(2, 11)) + [None]},\n",
    "    # For LinearRegression, you may not need to define any hyperparameters, or you can choose other relevant ones\n",
    "    'LinearRegression': {},\n",
    "    'VotingRegressor': {\n",
    "        'CatBoost__n_estimators': [1000], \n",
    "        'CatBoost__learning_rate': [0.05], \n",
    "        'XGBoost__n_estimators': [1000], \n",
    "        'XGBoost__learning_rate': [0.05], \n",
    "        'RandomForest__n_estimators': [2000], \n",
    "        'RandomForest__max_depth': [None]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Call the training function\n",
    "train_prediction_models(\"dataset/data\", train, test, model_names, models, hyperparameters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b0ebd4-5779-41e9-8ece-fbfe090fc134",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Définir les noms des modèles et les modèles eux-mêmes\n",
    "model_names = ['MLPRegressor']\n",
    "models = {\n",
    "    'MLPRegressor': MLPRegressor()\n",
    "}\n",
    "\n",
    "# Définir les hyperparamètres pour la recherche sur grille\n",
    "hyperparameters = {\n",
    "    'MLPRegressor': {\n",
    "        'hidden_layer_sizes': [(100,), (100,100)],\n",
    "        'alpha': [0.001],\n",
    "        'max_iter': [200],\n",
    "        'solver': ['adam']  # Utilisation explicite de l'optimiseur Adam\n",
    "    }\n",
    "}\n",
    "\n",
    "# Appel de la fonction\n",
    "train_prediction_models(\"dataset/data\", train, test, model_names, models, hyperparameters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064da843-4d6c-44dc-802d-62ba0c8bd004",
   "metadata": {},
   "source": [
    "# Elastic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7488c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------- \n",
      "TRAINING IN PROCESS FOR DISEASE : dataset/data\n",
      "----------------------------------------------------\n",
      "ElasticNet\n",
      "Hyperparameters: {'alpha': 1, 'l1_ratio': 0.8}\n",
      "Train RMSE: 0.37667140845113095\n",
      "Test RMSE: 0.40028055734684986\n",
      "Train RMSE: 547.5269436325527\n",
      "Test RMSE: 619.1318458271661\n",
      "Execution time: 1 min\n",
      "----------------------------------------------------\n",
      " \n",
      "Optimal hyperparameters of models\n",
      "{'ElasticNet__alpha': 1, 'ElasticNet__l1_ratio': 0.8}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# Définir les noms des modèles et les modèles eux-mêmes\n",
    "model_names = ['ElasticNet']\n",
    "models = {\n",
    "    'ElasticNet': ElasticNet()\n",
    "}\n",
    "\n",
    "# Définir les hyperparamètres pour la recherche sur grille\n",
    "hyperparameters = {\n",
    "    'ElasticNet': {'alpha': [1], 'l1_ratio': [0.8]}\n",
    "}\n",
    "\n",
    "# Appel de la fonction\n",
    "train_prediction_models(\"dataset/data\" , train, test, model_names, models, hyperparameters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd46b52-b128-473f-8293-2f7a41c6d775",
   "metadata": {},
   "source": [
    "# Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f7b2ea-b563-4b8f-baeb-65f42fa9b2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir les noms des modèles et les modèles eux-mêmes\n",
    "model_names = ['XGBoost']\n",
    "models = {'XGBoost': XGBRegressor()}\n",
    "\n",
    "# Définir les hyperparamètres pour la recherche sur grille\n",
    "hyperparameters = {'XGBoost': {'alpha': [0.6378671093136276],\n",
    " 'colsample_bytree': [0.717229517496302],\n",
    " 'gamma': [0.22950133682316104],\n",
    " 'learning_rate': [0.014810947307879158],\n",
    " 'max_depth': [5],\n",
    " 'min_child_weight': [2],\n",
    " 'subsample': [0.7507159458738991]}\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Appel de la fonction\n",
    "train_prediction_models(\"dataset/data\" , train, test, model_names, models, hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature selection from feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importance:\n",
      "Month 3: 216.0\n",
      "Month 2: 174.0\n",
      "Month 1: 170.0\n",
      "Final consumption expenditure (annual % growth): 22.0\n",
      "Exports of goods and services (annual % growth): 11.0\n",
      "Energy Price Index: 11.0\n",
      "Services, value added (annual % growth): 9.0\n",
      "population (2023): 8.0\n",
      "Industry (including construction), value added (annual % growth): 8.0\n",
      "Headline Consumer Price Index: 8.0\n",
      "Agriculture, forestry, and fishing, value added (annual % growth): 5.0\n",
      "GDP (current US$): 5.0\n",
      "Imports of goods and services (annual % growth): 3.0\n",
      "Infrastructure Score: 2.0\n",
      "Manufacturing, value added (annual % growth): 2.0\n",
      "Division proxy_Division-2: 2.0\n",
      "netChange: 1.0\n",
      "Timeliness Score: 1.0\n",
      "Tracking and Tracing Score: 1.0\n",
      "Division proxy_Division-1: 1.0\n",
      "Product Life cycel status_EOL: 1.0\n",
      "Product Life cycel status_active: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Load the pickled XGBoost model\n",
    "with open('model_XGBoost_1.pkl', 'rb') as file:\n",
    "    xgb_model = pickle.load(file)\n",
    "\n",
    "# Get feature importance\n",
    "feature_importance = xgb_model.get_booster().get_score(importance_type='weight')\n",
    "\n",
    "# Display feature importance\n",
    "print(\"Feature Importance:\")\n",
    "for feature, importance in sorted(feature_importance.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{feature}: {importance}\")\n",
    "\n",
    "# define a threshold\n",
    "seuil_importance = 4\n",
    "features_to_keep = [feature for feature, importance in feature_importance.items() if importance >= seuil_importance]\n",
    "\n",
    "# Filtrer les DataFrames d'entraînement et de test\n",
    "X_train_filtered = X_train[features_to_keep]\n",
    "X_test_filtered = X_test[features_to_keep]\n",
    "\n",
    "features_to_keep\n",
    "features_to_keep.append('Month 4')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Final modele using features selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------- \n",
      "TRAINING IN PROCESS FOR DISEASE : dataset/data\n",
      "----------------------------------------------------\n",
      "XGBoost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: {'alpha': 0.7, 'colsample_bytree': 0.717229517496302, 'gamma': 0.22950133682316104, 'learning_rate': 0.014810947307879158, 'max_depth': 6, 'min_child_weight': 2, 'subsample': 0.7507159458738991}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train RMSE Hfactory: 0.4955386618615962\n",
      "test RMSE Hfactory : 0.4320364163556861\n",
      "Train RMSE: 443.1148809737429\n",
      "Test RMSE: 586.3480769418557\n",
      "Execution time: 4 min\n",
      "----------------------------------------------------\n",
      " \n",
      "Optimal hyperparameters of models\n",
      "{'XGBoost__alpha': 0.7, 'XGBoost__colsample_bytree': 0.717229517496302, 'XGBoost__gamma': 0.22950133682316104, 'XGBoost__learning_rate': 0.014810947307879158, 'XGBoost__max_depth': 6, 'XGBoost__min_child_weight': 2, 'XGBoost__subsample': 0.7507159458738991}\n"
     ]
    }
   ],
   "source": [
    "# Définir les noms des modèles et les modèles eux-mêmes\n",
    "model_names = ['XGBoost']\n",
    "models = {'XGBoost': XGBRegressor()}\n",
    "\n",
    "# Définir les hyperparamètres pour la recherche sur grille\n",
    "hyperparameters = {'XGBoost': {'alpha': [0.70],\n",
    " 'colsample_bytree': [0.717229517496302],\n",
    " 'gamma': [0.22950133682316104],\n",
    " 'learning_rate': [0.014810947307879158],\n",
    " 'max_depth': [5],\n",
    " 'min_child_weight': [2],\n",
    " 'subsample': [0.7507159458738991]}\n",
    "}\n",
    "\n",
    "\n",
    "# Appel de la fonction\n",
    "train_prediction_models(\"dataset/data\" ,train[features_to_keep], test[features_to_keep], model_names, models, hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Creation of file for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "# import joblib  # Utilisez cette importation si vous préférez utiliser joblib pour charger le modèle\n",
    "\n",
    "# Charger les données de test normalisées\n",
    "X_test_avec_index = pd.read_csv('datasets/yann_test_avec_index.csv', sep=';')\n",
    "X_test_avec_index = X_test_avec_index.drop(['Unnamed: 0','Third_T1','Third_T2','Third_T3'], axis=1)\n",
    "index = X_test_avec_index[\"index\"].copy()\n",
    "X_test_sans_index = X_test_avec_index.drop(columns = 'index')\n",
    "\n",
    "# Créer un imputer qui remplacera les valeurs manquantes par la moyenne des colonnes\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "# Adapter l'imputer aux données et transformer les données\n",
    "X_test_sans_index_mean = imputer.fit_transform(X_test_sans_index)\n",
    "# Convertir le résultat en DataFrame et remettre 'Month 4' comme index\n",
    "X_test = pd.DataFrame(X_test_sans_index_mean, index=X_test_sans_index.index, columns=X_test_sans_index.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_train = pd.read_csv('datasets/yann/train_data.csv', sep=',').drop('Month 4', axis = 1)\n",
    "\n",
    "scaler.fit(train_train)\n",
    "\n",
    "# Transformer les données de test avec le même scaler\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convertir le résultat en DataFrame et remettre l'index\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, index=index, columns=X_test.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features_to_keep = ['Month 1',\n",
    " 'Month 2',\n",
    " 'Month 3',\n",
    " 'population (2023)',\n",
    " 'Agriculture, forestry, and fishing, value added (annual % growth)',\n",
    " 'Exports of goods and services (annual % growth)',\n",
    " 'Final consumption expenditure (annual % growth)',\n",
    " 'GDP (current US$)',\n",
    " 'Industry (including construction), value added (annual % growth)',\n",
    " 'Services, value added (annual % growth)',\n",
    " 'Energy Price Index',\n",
    " 'Headline Consumer Price Index']\n",
    "\n",
    "# Charger le modèle depuis le fichier pickle\n",
    "with open('datasets/yann/model_XGBoost_good.pkl', 'rb') as file:  # Remplacer par le chemin de votre modèle\n",
    "    model = pickle.load(file)\n",
    "    # ou avec joblib : model = joblib.load('path_to_your_model.pkl')\n",
    "\n",
    "    \n",
    "# Utiliser le modèle pour faire des prédictions\n",
    "predictions = model.predict(X_test_scaled[features_to_keep])\n",
    "\n",
    "# Afficher les prédictions\n",
    "print(predictions)\n",
    "\n",
    "\n",
    "predictions_df = pd.DataFrame(predictions, columns=['Month 4'])\n",
    "predictions_df['index'] = index\n",
    "predictions_df\n",
    "\n",
    "\n",
    "# Sauvegarder les prédictions dans un fichier CSV, 'Month 4' comme index\n",
    "predictions_df.to_csv('datasets/predictions.csv', sep=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(predictions_df['Month 4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"datasets/predictions.csv\", delimiter=\";\")\n",
    "df_test = df.drop(columns=[\"Unnamed: 0\"])\n",
    "df_test = df_test[[\"index\", \"Month 4\"]]\n",
    "df_test.to_csv(\"predictions_example_6666.csv\", sep=\";\" , index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
